# ⚡ Q3. CPU Usage Prediction with DVC + MLflow

This project demonstrates an **end-to-end MLOps pipeline** for predicting CPU usage using machine learning models.  
It uses **DVC** for reproducible pipelines and data versioning, and **MLflow** for experiment tracking & model management.

---

## 🔧 1. Environment Setup

Install dependencies:
```bash
pip install dvc[all] mlflow scikit-learn xgboost optuna matplotlib seaborn shap
# Optional: For neural networks
pip install torch
```

Initialize Git & DVC in your project folder:
```bash
git init
dvc init
```

## 📂 2. Project Structure

A clean layout inside VS Code:
```bash
cpu-usage-prediction/
│
├── data/
│   ├── raw.csv
│   └── processed.csv          # generated by preprocess.py
│
├── src/
│   ├── preprocess.py           # cleans raw.csv → processed.csv
│   ├── train.py                # trains model, logs metrics to MLflow
│   └── evaluate.py             # optional: extra evaluation & explainability
│
├── models/
│   └── model.pkl               # trained model artifacts
│
├── dvc.yaml                    # pipeline definition
├── dvc.lock                    # pipeline lock file (auto-generated)
├── requirements.txt
└── README.md
```

## 📊 3. Data Versioning (DVC)

Track your dataset:
```bash
dvc add data/raw.csv
git add data/raw.csv.dvc .gitignore
git commit -m "Add raw dataset"
```

If you have remote storage (Google Drive, S3, etc.):
```bash
dvc remote add -d storage <remote-url>
dvc push
```

## ⚙️ 4. DVC Pipeline

`dvc.yaml` defines the pipeline stages:
```yaml
stages:
  preprocess:
    cmd: python src/preprocess.py data/raw.csv data/processed.csv
    deps:
      - src/preprocess.py
      - data/raw.csv
    outs:
      - data/processed.csv

  train:
    cmd: python src/train.py data/processed.csv models/
    deps:
      - src/train.py
      - data/processed.csv
    outs:
      - models/model.pkl
```

Run the full pipeline:
```bash
dvc repro
```

## 📈 5. Experiment Tracking (MLflow)

Start MLflow UI:
```bash
mlflow ui --host 127.0.0.1 --port 5000 --workers 1
```

Open your browser at:
```
http://127.0.0.1:5000
```

You will see:
- Parameters (model type, hyperparameters)
- Metrics (RMSE, MAE, R²)
- Artifacts (trained models, plots, SHAP explanations)
- Model versions

## 🚀 6. Example Workflow

1. Add/update dataset (`data/raw.csv`).
2. Run pipeline with DVC:
   ```bash
   dvc repro
   ```
3. Track results in MLflow UI.
4. Push dataset & artifacts to remote:
   ```bash
   dvc push
   ```

## ✅ 7. Supported Models

- Linear Regression
- Random Forest
- XGBoost
- (Optional) MLP (PyTorch)

Easily extendable with Optuna hyperparameter tuning

## 📝 Notes

- `src/train.py` logs metrics & models to MLflow automatically.
- Change model type by modifying the command in `dvc.yaml`.
- Use Optuna inside `train.py` for auto-hyperparameter tuning.

## 🛠️ Implementation Files

### requirements.txt
```
dvc[all]
mlflow
scikit-learn
xgboost
optuna
matplotlib
seaborn
shap
pandas
numpy
torch
```

### dvc.yaml
```yaml
stages:
  preprocess:
    cmd: python src/preprocess.py data/raw.csv data/processed.csv
    deps:
      - src/preprocess.py
      - data/raw.csv
    outs:
      - data/processed.csv

  train:
    cmd: python src/train.py data/processed.csv models/ --model_type xgboost
    deps:
      - src/train.py
      - data/processed.csv
    outs:
      - models/model.pkl
    params:
      - model_type
      - hyperparameters

  evaluate:
    cmd: python src/evaluate.py models/model.pkl data/processed.csv
    deps:
      - src/evaluate.py
      - models/model.pkl
      - data/processed.csv
```

### params.yaml
```yaml
model_type: "xgboost"  # options: linear, random_forest, xgboost, mlp
hyperparameters:
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
  random_forest:
    n_estimators: 100
    max_depth: 10
  linear:
    alpha: 1.0
```

## 🔄 Pipeline Execution

1. **Data Preprocessing**: Cleans and prepares raw CPU usage data
2. **Model Training**: Trains selected model with hyperparameter tuning
3. **Model Evaluation**: Generates performance metrics and SHAP explanations
4. **Experiment Tracking**: Logs everything to MLflow for comparison

## 📊 MLflow Integration

The pipeline automatically tracks:
- **Parameters**: Model type, hyperparameters, data preprocessing steps
- **Metrics**: RMSE, MAE, R², training time
- **Artifacts**: Trained models, feature importance plots, SHAP visualizations
- **Models**: Versioned models ready for deployment

## 🎯 Getting Started

1. Clone this repository
2. Install dependencies: `pip install -r requirements.txt`
3. Initialize DVC: `dvc init`
4. Add your CPU usage data to `data/raw.csv`
5. Run the pipeline: `dvc repro`
6. View results: `mlflow ui`

## 🔍 Model Comparison

Use MLflow UI to compare different models and hyperparameters across experiments. The pipeline supports easy switching between model types and automatic hyperparameter optimization with Optuna.
